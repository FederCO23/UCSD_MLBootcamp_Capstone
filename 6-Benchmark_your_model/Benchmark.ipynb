{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import glob\n",
    "import random\n",
    "import imageio\n",
    "import imageio.v2 as imageio  # Explicitly use version 2 API\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import jaccard_score\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "#from realesrgan import RealESRGAN\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from scipy.ndimage import zoom\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "CUDA available: True\n",
      "GPU Model: NVIDIA GeForce GTX 1080 Ti\n",
      "CUDA Version: 11.8\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Device Properties:\n",
      "  Name: NVIDIA GeForce GTX 1080 Ti\n",
      "  Total Memory (GB): 10.999755859375\n",
      "  Multiprocessors: 28\n",
      "  Compute Capability: 6 . 1\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(70*'-')\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "current_device = torch.cuda.current_device()\n",
    "print(\"GPU Model:\", torch.cuda.get_device_name(current_device))\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(70*'-')\n",
    "\n",
    "device_properties = torch.cuda.get_device_properties(current_device)\n",
    "print(\"\\nDevice Properties:\")\n",
    "print(\"  Name:\", device_properties.name)\n",
    "print(\"  Total Memory (GB):\", device_properties.total_memory / (1024 ** 3))  # Convert bytes to GB\n",
    "print(\"  Multiprocessors:\", device_properties.multi_processor_count)\n",
    "print(\"  Compute Capability:\", device_properties.major, \".\", device_properties.minor)\n",
    "print(70*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before importing the dataset, apply Super Resolution to the source images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the numeric value from filenames\n",
    "def numeric_sort_key(filepath):\n",
    "    # Extract numbers from the filename using a regular expression\n",
    "    match = re.search(r'\\d+', filepath)\n",
    "    # Return the integer value of the number if found, otherwise 0\n",
    "    return int(match.group()) if match else 0\n",
    "    \n",
    "def preprocess_dataset(image_dir, mask_dir, output_dir, scale_factor=2):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by applying super-resolution and saving the results using rasterio.\n",
    "    \n",
    "    Args:\n",
    "        image_dir (str): Directory containing input images.\n",
    "        mask_dir (str): Directory containing mask images.\n",
    "        output_dir (str): Base directory to store the preprocessed data.\n",
    "        scale_factor (int): Factor by which to upscale the images.\n",
    "    \"\"\"\n",
    "    # Get all the image and mask paths and sort them numerically\n",
    "    image_paths = sorted(glob.glob(f\"{image_dir}/*.tif\"), key=numeric_sort_key)\n",
    "    mask_paths = sorted(glob.glob(f\"{mask_dir}/*.tif\"), key=numeric_sort_key)\n",
    "\n",
    "    # Create output directories for images and masks\n",
    "    images_output_dir = Path(output_dir) / \"input\"\n",
    "    masks_output_dir = Path(output_dir) / \"labels\"\n",
    "    images_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    masks_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Process images\n",
    "    print(f\"Processing {len(image_paths)} images...\")\n",
    "    for img_path in image_paths:\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read and upscale each band\n",
    "            upscaled_data = []\n",
    "            for band in range(1, src.count + 1):  # Loop through bands\n",
    "                band_data = src.read(band)\n",
    "                upscaled_band = zoom(band_data, scale_factor, order=3)  # Cubic interpolation\n",
    "                upscaled_data.append(upscaled_band)\n",
    "            \n",
    "            # Write to a new file\n",
    "            meta = src.meta.copy()\n",
    "            meta.update({\n",
    "                \"height\": int(src.height * scale_factor),\n",
    "                \"width\": int(src.width * scale_factor),\n",
    "                \"transform\": src.transform * rasterio.Affine.scale(1 / scale_factor),\n",
    "            })\n",
    "            output_path = images_output_dir / os.path.basename(img_path)\n",
    "            with rasterio.open(output_path, \"w\", **meta) as dst:\n",
    "                for i, band_data in enumerate(upscaled_data, start=1):\n",
    "                    dst.write(band_data, i)\n",
    "    \n",
    "    # Process masks\n",
    "    print(f\"Processing {len(mask_paths)} masks...\")\n",
    "    for mask_path in mask_paths:\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask_data = src.read(1)  # Single-band mask\n",
    "            upscaled_mask = zoom(mask_data, scale_factor, order=0)  # Nearest-neighbor interpolation\n",
    "            \n",
    "            # Write to a new file\n",
    "            meta = src.meta.copy()\n",
    "            meta.update({\n",
    "                \"height\": int(src.height * scale_factor),\n",
    "                \"width\": int(src.width * scale_factor),\n",
    "                \"transform\": src.transform * rasterio.Affine.scale(1 / scale_factor),\n",
    "            })\n",
    "            output_path = masks_output_dir / os.path.basename(mask_path)\n",
    "            with rasterio.open(output_path, \"w\", **meta) as dst:\n",
    "                dst.write(upscaled_mask, 1)\n",
    "\n",
    "    print(f\"Preprocessing complete. Data saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_dataset(\n",
    "#     image_dir = '../5-Data_Wrangling/data_split2/train/input/', \n",
    "#     mask_dir = '../5-Data_Wrangling/data_split2/train/labels/',\n",
    "#     output_dir = '../5-Data_Wrangling/data_split2sr/train/',\n",
    "#     scale_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_dataset(\n",
    "#     image_dir = '../5-Data_Wrangling/data_split2/val/input/', \n",
    "#     mask_dir = '../5-Data_Wrangling/data_split2/val/labels/',\n",
    "#     output_dir = '../5-Data_Wrangling/data_split2sr/val/',\n",
    "#     scale_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_dataset(\n",
    "#     image_dir = '../5-Data_Wrangling/data_split2/test/input/', \n",
    "#     mask_dir = '../5-Data_Wrangling/data_split2/test/labels/',\n",
    "#     output_dir = '../5-Data_Wrangling/data_split2sr/test/',\n",
    "#     scale_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "- Import folder with images\n",
    "- Import folder with masks\n",
    "- Create list with training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the image and mask paths and sort them numerically\n",
    "folder_data_train = sorted(\n",
    "    glob.glob(\"../5-Data_Wrangling/data_split2sr/train/input/*tif\"),\n",
    "    key=numeric_sort_key\n",
    ")\n",
    "folder_mask_train = sorted(\n",
    "    glob.glob(\"../5-Data_Wrangling/data_split2sr/train/labels/*tif\"),\n",
    "    key=numeric_sort_key\n",
    ")\n",
    "\n",
    "folder_data_val = sorted(\n",
    "    glob.glob(\"../5-Data_Wrangling/data_split2sr/val/input/*tif\"),\n",
    "    key=numeric_sort_key\n",
    ")\n",
    "folder_mask_val = sorted(\n",
    "    glob.glob(\"../5-Data_Wrangling/data_split2sr/val/labels/*tif\"),\n",
    "    key=numeric_sort_key\n",
    ")\n",
    "\n",
    "folder_data_test = sorted(\n",
    "    glob.glob(\"../5-Data_Wrangling/data_split2sr/test/input/*tif\"),\n",
    "    key=numeric_sort_key\n",
    ")\n",
    "folder_mask_test = sorted(\n",
    "    glob.glob(\"../5-Data_Wrangling/data_split2sr/test/labels/*tif\"),\n",
    "    key=numeric_sort_key\n",
    ")\n",
    "\n",
    "# # Assign to variables\n",
    "# train_image_paths = folder_data_train[:]\n",
    "# val_image_paths = folder_data_val[:]\n",
    "# test_image_paths = folder_data_test[:]\n",
    "\n",
    "# train_mask_paths = folder_mask_train[:]\n",
    "# val_mask_paths = folder_mask_val[:]\n",
    "# test_mask_paths = folder_mask_test[:]\n",
    "\n",
    "# debugging\n",
    "train_image_paths = folder_data_train[:100]\n",
    "val_image_paths = folder_data_val[:12]\n",
    "test_image_paths = folder_data_test[:12]\n",
    "\n",
    "train_mask_paths = folder_mask_train[:100]\n",
    "val_mask_paths = folder_mask_val[:12]\n",
    "test_mask_paths = folder_mask_test[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control number of images by folder:\n",
      "========================================\n",
      "\t\tinput: \tlabels:\n",
      "Train\t\t201\t201\t73.90%\n",
      "Val\t\t35\t35\t12.87%\n",
      "Test\t\t36\t36\t13.24%\n",
      "========================================\n",
      "Total\t\t272\t272\n"
     ]
    }
   ],
   "source": [
    "# Check the number of train, val and test images\n",
    "print('\\nControl number of images by folder:')\n",
    "print(40*'=')\n",
    "input_total = len(folder_data_train)+len(folder_data_val)+len(folder_data_test)\n",
    "labels_total = len(folder_mask_train)+len(folder_mask_val)+len(folder_mask_test)\n",
    "print('\\t\\tinput: \\tlabels:')\n",
    "print(f'Train\\t\\t{len(folder_data_train)}\\t{len(folder_mask_train)}\\t{(len(folder_data_train)/input_total)*100:.2f}%')\n",
    "print(f'Val\\t\\t{len(folder_data_val)}\\t{len(folder_mask_val)}\\t{(len(folder_data_val)/input_total)*100:.2f}%')\n",
    "print(f'Test\\t\\t{len(folder_data_test)}\\t{len(folder_mask_test)}\\t{(len(folder_data_test)/input_total)*100:.2f}%')\n",
    "print(40*'=')\n",
    "print(f'Total\\t\\t{input_total}\\t{labels_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(image_paths, num_samples=5):\n",
    "    \"\"\"\n",
    "    Plot histograms for each band (R, G, B, NIR) for a given sample of images.\n",
    "    \"\"\"\n",
    "    red_values = []\n",
    "    green_values = []\n",
    "    blue_values = []\n",
    "    nir_values = []\n",
    "    \n",
    "    # Iterate over a subset of images\n",
    "    for img_path in image_paths[:num_samples]:  \n",
    "        image = imageio.imread(img_path)\n",
    "        \n",
    "        # Separate the bands\n",
    "        red_values.extend(image[:, :, 0].flatten())\n",
    "        green_values.extend(image[:, :, 1].flatten())\n",
    "        blue_values.extend(image[:, :, 2].flatten())\n",
    "        nir_values.extend(image[:, :, 3].flatten())\n",
    "    \n",
    "    # Plot histograms\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(red_values, bins=50, color='red', alpha=0.7)\n",
    "    plt.title(\"Red Band Histogram\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(green_values, bins=50, color='green', alpha=0.7)\n",
    "    plt.title(\"Green Band Histogram\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(blue_values, bins=50, color='blue', alpha=0.7)\n",
    "    plt.title(\"Blue Band Histogram\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(nir_values, bins=50, color='purple', alpha=0.7)\n",
    "    plt.title(\"NIR Band Histogram\")\n",
    "    plt.xlabel(\"Pixel Intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your training image paths\n",
    "#plot_histograms(train_image_paths, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_balance_from_paths(mask_paths):\n",
    "    \"\"\"\n",
    "    Calculate the class balance (proportion of positive and negative pixels) for a dataset using mask file paths.\n",
    "    Args:\n",
    "        mask_paths: List of file paths to mask images.\n",
    "    Returns:\n",
    "        class_balance: Dictionary with the proportion of positive and negative pixels.\n",
    "    \"\"\"\n",
    "    total_pixels = 0\n",
    "    positive_pixels = 0\n",
    "\n",
    "    for mask_path in mask_paths:\n",
    "        mask = imageio.imread(mask_path).astype(np.float32)\n",
    "        mask = np.where(mask > 0.5, 1, 0)\n",
    "        \n",
    "        total_pixels += mask.size\n",
    "        positive_pixels += mask.sum()\n",
    "\n",
    "    negative_pixels = total_pixels - positive_pixels\n",
    "\n",
    "    # Calculate proportions\n",
    "    positive_ratio = positive_pixels / total_pixels\n",
    "    negative_ratio = negative_pixels / total_pixels\n",
    "\n",
    "    class_balance = {\n",
    "        \"positive_ratio\": positive_ratio,\n",
    "        \"negative_ratio\": negative_ratio,\n",
    "        \"total_pixels\": total_pixels,\n",
    "    }\n",
    "\n",
    "    return class_balance\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Balance in Training Set: {'positive_ratio': 0.041524200439453124, 'negative_ratio': 0.9584757995605468, 'total_pixels': 26214400}\n",
      "Class Balance in Validation Set: {'positive_ratio': 0.045077006022135414, 'negative_ratio': 0.9549229939778646, 'total_pixels': 3145728}\n",
      "Class Balance in Test Set: {'positive_ratio': 0.1297022501627604, 'negative_ratio': 0.8702977498372396, 'total_pixels': 3145728}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the training, validation, and test sets class balance\n",
    "train_balance = calculate_class_balance_from_paths(train_mask_paths)\n",
    "val_balance = calculate_class_balance_from_paths(val_mask_paths)\n",
    "test_balance = calculate_class_balance_from_paths(test_mask_paths)\n",
    "\n",
    "print(\"Class Balance in Training Set:\", train_balance)\n",
    "print(\"Class Balance in Validation Set:\", val_balance)\n",
    "print(\"Class Balance in Test Set:\", test_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization: Calculate Mean and StdDev from the training set (consider the 4-bands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean per band (R, G, B, NIR): [ 927.72309094  740.12511994  492.37997292 2441.66539495]\n",
      "Std deviation per band (R, G, B, NIR): [419.47567552 257.08428302 200.48893916 499.48728449]\n"
     ]
    }
   ],
   "source": [
    "# For Image's normalization: Calculate Mean and Standard Deviation for all the training set\n",
    "\n",
    "train_image_paths = glob.glob(\"../5-Data_Wrangling/data_split2sr/train/input/*.tif\")\n",
    "\n",
    "# Initialize arrays to accumulate mean and std values for each band\n",
    "mean_values = np.zeros(4)\n",
    "std_values = np.zeros(4)\n",
    "n_pixels = 0  # Total pixel count across all images (for averaging)\n",
    "\n",
    "# Iterate over all images in the training set\n",
    "for img_path in train_image_paths:\n",
    "    # Load the image and cast to float32 for precision\n",
    "    image = imageio.imread(img_path).astype(np.float32)\n",
    "    \n",
    "    # Calculate the mean and std per band for this image\n",
    "    mean_per_image = image.mean(axis=(0, 1))  # Mean across spatial dimensions\n",
    "    std_per_image = image.std(axis=(0, 1))    # Std deviation across spatial dimensions\n",
    "    \n",
    "    # Accumulate the total mean and std values\n",
    "    mean_values += mean_per_image\n",
    "    std_values += std_per_image\n",
    "    n_pixels += image.shape[0] * image.shape[1]  # Accumulate pixel count for averaging\n",
    "\n",
    "# Average the mean and std across all images\n",
    "mean_values /= len(train_image_paths)\n",
    "std_values /= len(train_image_paths)\n",
    "\n",
    "print(\"Mean per band (R, G, B, NIR):\", mean_values)\n",
    "print(\"Std deviation per band (R, G, B, NIR):\", std_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Resolution using CV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_super_resolution_opencv(image, scale_factor=2, interpolation=cv2.INTER_CUBIC):\n",
    "#     \"\"\"\n",
    "#     Apply OpenCV interpolation to enhance image resolution.\n",
    "#     Args:\n",
    "#         image (numpy array): Input image as a numpy array.\n",
    "#         scale_factor (int): Factor by which to upscale the image.\n",
    "#         interpolation: Interpolation method (default is cv2.INTER_CUBIC).\n",
    "#     Returns:\n",
    "#         numpy array: Upscaled image.\n",
    "#     \"\"\"\n",
    "#     height, width = image.shape[:2]\n",
    "#     new_dimensions = (width * scale_factor, height * scale_factor)\n",
    "#     upscaled_image = cv2.resize(image, new_dimensions, interpolation=interpolation)\n",
    "#     return upscaled_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, image_paths, target_paths, transform=None, band=None, device='cuda'):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.transform = transform\n",
    "        self.band = band # Specify which band to use (0: R, 1: G, 2: B, 3: NIR, None: all bands)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.device = device  # Specify the device for preloading\n",
    "\n",
    "        # Preload images and masks into GPU memory\n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Preloading images and masks into GPU memory...\")\n",
    "        # self.images = [torch.tensor(cv2.imread(path, cv2.IMREAD_UNCHANGED).astype(np.float32), device=self.device) for path in image_paths]\n",
    "        # self.masks = [torch.tensor(cv2.imread(path, cv2.IMREAD_UNCHANGED).astype(np.float32), device=self.device) for path in target_paths]\n",
    "        self.images = image_paths\n",
    "        self.masks = target_paths\n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Preloading complete.\")\n",
    "\n",
    "        \n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        # Debugging: Print dataset lengths and current index\n",
    "        print(f\"[DEBUG] Dataset size: {len(self.image_paths)} images, {len(self.masks)} masks\")\n",
    "        print(f\"[DEBUG] Current index: {index}\")\n",
    "    \n",
    "        # Access the paths safely\n",
    "        try:\n",
    "            # Get the current timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "             # Log the index being loaded\n",
    "            print(f\"[{timestamp}] Loading image and mask for index {index}...\")\n",
    "            \n",
    "            # Retrieve preloaded image and mask\n",
    "            # image = self.images[index]\n",
    "            # mask = self.masks[index]\n",
    "             # Debugging: Add print statements for image and mask paths\n",
    "            print(f\"Loading image: {self.image_paths[index]}\")\n",
    "            print(f\"Loading mask: {self.masks[index]}\")\n",
    "            image = torch.tensor(cv2.imread(self.images[index], cv2.IMREAD_UNCHANGED).astype(np.float32))\n",
    "            mask = torch.tensor(cv2.imread(self.masks[index], cv2.IMREAD_UNCHANGED).astype(np.float32))\n",
    "            print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\n",
    "            print(f\"Mask shape: {mask.shape}, dtype: {mask.dtype}\")\n",
    "               \n",
    "            # Select a specific band if specified\n",
    "            if self.band is not None:\n",
    "                image = image[:, :, self.band].unsqueeze(2)  # Add channel dimension\n",
    "\n",
    "                \n",
    "            # Normalize the image\n",
    "            # image_reshaped = image.reshape(-1, image.shape[-1])\n",
    "            # image_scaled = self.scaler.fit_transform(image_reshaped)\n",
    "            # image = image_scaled.reshape(image.shape)\n",
    "\n",
    "             # Normalize the image (Min-Max normalization)\n",
    "            image_min = image.min()\n",
    "            image_max = image.max()\n",
    "            if image_max > image_min:  # Avoid division by zero\n",
    "                image = (image - image_min) / (image_max - image_min)\n",
    "\n",
    "            # Reshape for MinMaxScaler and apply normalization\n",
    "            #image_reshaped = image.reshape(-1, 4)\n",
    "            #image_scaled = self.scaler.fit_transform(image_reshaped)\n",
    "            #image = image_scaled.reshape(image.shape)\n",
    "            \n",
    "            # Load the 1-band binary mask\n",
    "            # mask = imageio.imread(self.target_paths[index])\n",
    "            # mask = np.asarray(mask, dtype='float32')\n",
    "            #mask = np.where(mask>1, 0, mask) # some images has soil annotations as well\n",
    "            mask = torch.where(mask > 1, torch.tensor(0.0, device=self.device), mask)\n",
    "    \n",
    "            # Debugging: Print shapes and types\n",
    "            #print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\n",
    "            #print(f\"Mask shape: {mask.shape}, dtype: {mask.dtype}\")\n",
    "\n",
    "            # Debugging: Log the properties of the loaded image and mask\n",
    "            print(f\"[{timestamp}] Image shape: {image.shape}, dtype: {image.dtype}\")\n",
    "            print(f\"[{timestamp}] Mask shape: {mask.shape}, dtype: {mask.dtype}\")\n",
    "\n",
    "    \n",
    "            # Apply the transformation to both image and mask if self.transform is set\n",
    "            if self.transform:\n",
    "                try:\n",
    "                    image, mask = self.transform(image, mask)  # Pass both to transform if synchronized\n",
    "                except Exception as e:\n",
    "                    print(f\"[{timestamp}] Error during transformation at index {index}: {e}\")\n",
    "                    raise e\n",
    "    \n",
    "            return image, mask\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Error at index {index}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "\n",
    "# class CustomDataset(data.Dataset):\n",
    "#     def __init__(self, image_paths, target_paths, transform=None, band=None, apply_super_resolution=False, scale_factor=2):\n",
    "\n",
    "#         self.image_paths = image_paths\n",
    "#         self.target_paths = target_paths\n",
    "#         self.transform = transform\n",
    "#         self.band = band # Specify which band to use (0: R, 1: G, 2: B, 3: NIR, None: all bands)\n",
    "#         self.scaler = MinMaxScaler()\n",
    "#         self.apply_super_resolution = apply_super_resolution\n",
    "#         self.scale_factor = scale_factor  # Scaling factor for super-resolution\n",
    "#         #self.sr_model = sr_model\n",
    "#         #self.res_printed = False\n",
    "                \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         image = imageio.imread(self.image_paths[index]).astype(np.float32)\n",
    "\n",
    "#         # Select a specific band if specified\n",
    "#         if self.band is not None:\n",
    "#             image = image[:, :, self.band] #Select only the specified band\n",
    "#             image = image[:, :, np.newaxis]\n",
    "            \n",
    "#         # Load the 1-band binary mask\n",
    "#         mask = imageio.imread(self.target_paths[index])\n",
    "#         mask = np.asarray(mask, dtype='float32')\n",
    "#         mask = np.where(mask>1, 0, mask) # some images has soil annotations as well\n",
    "\n",
    "#         # Apply super-resolution if enabled\n",
    "#         if self.apply_super_resolution:\n",
    "#             image = apply_super_resolution_opencv(image, scale_factor=self.scale_factor, interpolation=cv2.INTER_CUBIC)\n",
    "#             mask = apply_super_resolution_opencv(mask, scale_factor=self.scale_factor, interpolation=cv2.INTER_NEAREST)  # Use INTER_NEAREST for masks\n",
    "    \n",
    "#         # Normalize the image\n",
    "#         image_reshaped = image.reshape(-1, image.shape[-1])\n",
    "#         image_scaled = self.scaler.fit_transform(image_reshaped)\n",
    "#         image = image_scaled.reshape(image.shape)\n",
    "\n",
    "#         # # Convert to PyTorch tensor\n",
    "#         # image_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
    "\n",
    "#         # # Apply super-resolution if enabled\n",
    "#         # if self.apply_super_resolution and self.sr_model:\n",
    "#         #     with torch.no_grad():\n",
    "#         #         image_tensor = self.sr_model(image_tensor).squeeze(0)  # Remove batch dimension\n",
    "#         #         if not self.res_printed:\n",
    "#         #             print(f'image resolution after super res: {image_tensor.shape}')\n",
    "#         #             self.res_printed = True\n",
    "                            \n",
    "        \n",
    "#         # Debugging: Print shapes and types\n",
    "#         #print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\n",
    "#         #print(f\"Mask shape: {mask.shape}, dtype: {mask.dtype}\")\n",
    "\n",
    "#         # Apply the transformation to both image and mask if self.transform is set\n",
    "#         if self.transform:\n",
    "#             image, mask = self.transform(image, mask)  # Pass both to transform if synchronized\n",
    "\n",
    "#         return image, mask\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynchronizedTransform:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # Convert tensors to NumPy arrays if needed\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.cpu().numpy()  # Move tensor to CPU and convert to NumPy array\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.cpu().numpy()  # Move tensor to CPU and convert to NumPy array\n",
    "        \n",
    "        # Convert numpy arrays to PIL images\n",
    "        if image.ndim == 3 and image.shape[-1] == 1:  # Single-band images\n",
    "            image = image.squeeze(-1)  # Remove the singleton channel\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))  # Scale to 0-255\n",
    "        mask = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            if isinstance(transform, transforms.RandomHorizontalFlip):\n",
    "                if torch.rand(1).item() < 0.5:\n",
    "                    image = TF.hflip(image)\n",
    "                    mask = TF.hflip(mask)\n",
    "            elif isinstance(transform, transforms.RandomVerticalFlip):\n",
    "                if torch.rand(1).item() < 0.5:\n",
    "                    image = TF.vflip(image)\n",
    "                    mask = TF.vflip(mask)\n",
    "            elif isinstance(transform, transforms.RandomChoice):\n",
    "                angle = random.choice([0, 90, 180, 270])\n",
    "                image = TF.rotate(image, angle)\n",
    "                mask = TF.rotate(mask, angle)\n",
    "            elif isinstance(transform, transforms.ToTensor):\n",
    "                image = TF.to_tensor(image)\n",
    "                mask = TF.to_tensor(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# class SynchronizedTransform:\n",
    "#     def __init__(self, transforms):\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __call__(self, image, mask):\n",
    "#         # Convert tensors to NumPy arrays if needed\n",
    "#         if isinstance(image, torch.Tensor):\n",
    "#             image = image.numpy()  # Convert tensor to NumPy array\n",
    "#         if isinstance(mask, torch.Tensor):\n",
    "#             mask = mask.numpy()  # Convert tensor to NumPy array\n",
    "        \n",
    "        \n",
    "#         # Convert numpy arrays to PIL images\n",
    "#         #image = Image.fromarray((image * 255).astype(np.uint8))  # Scale to 0-255 for display\n",
    "\n",
    "#         # Convert numpy arrays to PIL images\n",
    "#         if image.ndim == 3 and image.shape[-1] == 1:  # Single-band images\n",
    "#             image = image.squeeze(-1)  # Remove the singleton channel\n",
    "\n",
    "#         # Conver to PIL images\n",
    "#         image = Image.fromarray((image * 255).astype(np.uint8))  # Scale to 0-255\n",
    "#         mask = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "\n",
    "        \n",
    "#         for transform in self.transforms:\n",
    "#             if isinstance(transform, transforms.RandomHorizontalFlip):\n",
    "#                 if torch.rand(1).item() < 0.5:\n",
    "#                     image = TF.hflip(image)\n",
    "#                     mask = TF.hflip(mask)\n",
    "#             elif isinstance(transform, transforms.RandomVerticalFlip):\n",
    "#                 if torch.rand(1).item() < 0.5:\n",
    "#                     image = TF.vflip(image)\n",
    "#                     mask = TF.vflip(mask)\n",
    "#             elif isinstance(transform, transforms.RandomChoice):\n",
    "#                 # Pick a random rotation angle from the specified options\n",
    "#                 angle = random.choice([0, 90, 180, 270])\n",
    "#                 image = TF.rotate(image, angle)\n",
    "#                 mask = TF.rotate(mask, angle)\n",
    "#             elif isinstance(transform, transforms.ToTensor):\n",
    "#                 image = TF.to_tensor(image)\n",
    "#                 mask = TF.to_tensor(mask)\n",
    "\n",
    "#         return image, mask\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 15:00:46] Preloading images and masks into GPU memory...\n",
      "[2024-12-08 15:00:46] Preloading complete.\n",
      "[2024-12-08 15:00:46] Preloading images and masks into GPU memory...\n",
      "[2024-12-08 15:00:46] Preloading complete.\n",
      "[2024-12-08 15:00:46] Preloading images and masks into GPU memory...\n",
      "[2024-12-08 15:00:46] Preloading complete.\n"
     ]
    }
   ],
   "source": [
    "simple_transform = SynchronizedTransform([\n",
    "    transforms.ToTensor()  # Convert both image and mask to tensor\n",
    "])\n",
    "\n",
    "# augmentation_transform = SynchronizedTransform([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.RandomChoice([\n",
    "#         transforms.RandomRotation(0),\n",
    "#         transforms.RandomRotation(90),\n",
    "#         transforms.RandomRotation(180),\n",
    "#         transforms.RandomRotation(270)\n",
    "#     ]),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "\n",
    "augmentation_transform = SynchronizedTransform([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomChoice([\n",
    "    #     transforms.RandomRotation(0),\n",
    "    #     transforms.RandomRotation(90),\n",
    "    #     transforms.RandomRotation(180),\n",
    "    #     transforms.RandomRotation(270)\n",
    "    # ]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load a pre-trained super-resolution model (e.g., SRResNet)\n",
    "# sr_model = super_resolution.srresnet_1x().to(DEVICE)\n",
    "# sr_model.eval()\n",
    "\n",
    "use_all_bands = False\n",
    "model_band = 3 # 0:red, 1:green, 2:blue, 3:NIR, None:all 4 bands\n",
    "\n",
    "train_dataset = CustomDataset(train_image_paths, train_mask_paths, transform=augmentation_transform, band=model_band)\n",
    "val_dataset = CustomDataset(val_image_paths, val_mask_paths, transform=simple_transform, band=model_band)\n",
    "test_dataset = CustomDataset(test_image_paths, test_mask_paths, transform=simple_transform, band=model_band)\n",
    "\n",
    "# train_dataset = CustomDataset(train_image_paths, train_mask_paths, transform=augmentation_transform, band=model_band)\n",
    "# val_dataset = CustomDataset(val_image_paths, val_mask_paths, transform=simple_transform, band=model_band)\n",
    "# test_dataset = CustomDataset(test_image_paths, test_mask_paths, transform=simple_transform, band=model_band)\n",
    "\n",
    "# image, mask = train_dataset[0]  # Load the first item\n",
    "# print(f\"Transformed image shape: {image.shape}, dtype: {image.dtype}\")\n",
    "# print(f\"Transformed mask shape: {mask.shape}, dtype: {mask.dtype}\")\n",
    "\n",
    "# train_dataset = CustomDataset(\n",
    "#     train_image_paths, \n",
    "#     train_mask_paths, \n",
    "#     transform=augmentation_transform, \n",
    "#     band=model_band, \n",
    "#     apply_super_resolution = True,\n",
    "#     scale_factor = 2)\n",
    "\n",
    "# val_dataset = CustomDataset(\n",
    "#     val_image_paths, \n",
    "#     val_mask_paths, \n",
    "#     transform=simple_transform, \n",
    "#     band=model_band, \n",
    "#     apply_super_resolution = True,\n",
    "#     scale_factor = 2)\n",
    "\n",
    "# test_dataset = CustomDataset(\n",
    "#     test_image_paths, \n",
    "#     test_mask_paths, \n",
    "#     transform=simple_transform, \n",
    "#     band=model_band, \n",
    "#     apply_super_resolution = True,\n",
    "#     scale_factor = 2)\n",
    "\n",
    "# image, mask = train_dataset[0]  # Load the first item\n",
    "# print(f\"Transformed image shape: {image.shape}, dtype: {image.dtype}\")\n",
    "# print(f\"Transformed mask shape: {mask.shape}, dtype: {mask.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the synchronization of Data Augmentation transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image, mask = dataset[i]\n",
    "        \n",
    "        # Display input image and mask side-by-side\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image.permute(1, 2, 0))  # Convert CHW to HWC for display\n",
    "        plt.title(\"Augmented Image\")\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask[0], cmap='gray')  # Show mask in grayscale\n",
    "        plt.title(\"Augmented Mask\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 15:00:48] Loading image and mask for index 0...\n",
      "Loading image: ../5-Data_Wrangling/data_split2sr/train/input\\img(100).tif\n",
      "Loading mask: ../5-Data_Wrangling/data_split2sr/train/labels\\target(100).tif\n",
      "Image shape: torch.Size([512, 512, 4]), dtype: torch.float32\n",
      "Mask shape: torch.Size([512, 512]), dtype: torch.float32\n",
      "[2024-12-08 15:00:48] Error at index 0: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_augmentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m, in \u001b[0;36mvisualize_augmentations\u001b[1;34m(dataset, num_samples)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_augmentations\u001b[39m(dataset, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[1;32m----> 3\u001b[0m         image, mask \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Display input image and mask side-by-side\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[32], line 88\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Error at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[32], line 65\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     54\u001b[0m     image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m image_min) \u001b[38;5;241m/\u001b[39m (image_max \u001b[38;5;241m-\u001b[39m image_min)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Reshape for MinMaxScaler and apply normalization\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#image_reshaped = image.reshape(-1, 4)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#image_scaled = self.scaler.fit_transform(image_reshaped)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# mask = np.asarray(mask, dtype='float32')\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#mask = np.where(mask>1, 0, mask) # some images has soil annotations as well\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Debugging: Print shapes and types\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#print(f\"Mask shape: {mask.shape}, dtype: {mask.dtype}\")\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Debugging: Log the properties of the loaded image and mask\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Image shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "visualize_augmentations(train_dataset, num_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../5-Data_Wrangling/data_split2sr/train/input\\\\img(100).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1001).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1002).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1003).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1004).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1005).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1006).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1007).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1008).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1009).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(101).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1010).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1011).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1012).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1013).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1014).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1015).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1016).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1017).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/input\\\\img(1018).tif'],\n",
       " ['../5-Data_Wrangling/data_split2sr/train/labels\\\\target(100).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1001).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1002).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1003).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1004).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1005).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1006).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1007).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1008).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1009).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(101).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1010).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1011).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1012).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1013).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1014).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1015).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1016).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1017).tif',\n",
       "  '../5-Data_Wrangling/data_split2sr/train/labels\\\\target(1018).tif'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.image_paths[:20], train_dataset.target_paths[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    'train': train_dataset, 'val': val_dataset, 'test': test_dataset\n",
    "}\n",
    "\n",
    "#dataloaders = {\n",
    "#    'train': torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0),\n",
    "#    'val': torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0),\n",
    "#    'test': torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=0)\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'efficientnet-b7'\n",
    "#ENCODER = 'efficientnet-b0' #for DeepLabV3+\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['solar_panel']\n",
    "ACTIVATION = 'sigmoid'\n",
    "#DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#model = smp.Unet(\n",
    "#model = smp.DeepLabV3Plus(\n",
    "model = smp.UnetPlusPlus(\n",
    "    in_channels = 4 if use_all_bands else 1, #4 for all bands\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    #decoder_atrous_rates = (6, 12, 24) # for DeepLabv3+\n",
    "    #decoder_dropout=0.3\n",
    "       \n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Devs\\pyEnv-1\\UCSD_MLBootcamp_Capstone\\5b-Run_a_Model\\5b-Model-env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, pos_weight=1.0, neg_weight=1.0):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight  # Weight for positive class\n",
    "        self.neg_weight = neg_weight  # Weight for negative class\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # print(f\"weights: pos {self.pos_weight} neg {self.neg_weight}\") # debugging\n",
    "        # Weighted BCE computation\n",
    "        loss = -self.pos_weight * targets * torch.log(inputs + 1e-7) - \\\n",
    "               self.neg_weight * (1 - targets) * torch.log(1 - inputs + 1e-7)\n",
    "        return loss.mean()\n",
    "\n",
    "class BCEFocalNegativeIoULoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=1.5, pos_weight=2.0, neg_weight=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight for Focal Loss.\n",
    "            gamma: Modulating factor for Focal Loss.\n",
    "            pos_weight: Weight for positive class in BCE Loss.\n",
    "            neg_weight: Weight for negative class in BCE Loss.\n",
    "        \"\"\"\n",
    "        super(BCEFocalNegativeIoULoss, self).__init__()\n",
    "        self.bce = WeightedBCELoss(pos_weight=pos_weight, neg_weight=neg_weight)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def focal_loss(self, inputs, targets):\n",
    "        BCE_loss = -targets * torch.log(inputs + 1e-7) - (1 - targets) * torch.log(1 - inputs + 1e-7)\n",
    "        #pt = torch.exp(-BCE_loss)  # Probability of the true class\n",
    "        pt = inputs * targets + (1 - inputs) * (1 - targets)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if targets.sum() == 0:  # Skip blank masks\n",
    "            return torch.tensor(0.0, requires_grad=True).to(inputs.device)\n",
    "\n",
    "        # Core loss components\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        focal_loss = self.focal_loss(inputs, targets)\n",
    "\n",
    "        # Compute IoU for positive and negative classes\n",
    "        #preds = inputs.sigmoid()  # Apply sigmoid activation\n",
    "        preds = inputs\n",
    "        #preds = model(inputs)  # No need to apply sigmoid again if already applied in the model\n",
    "        iou_positive, mean_iou = compute_class_aware_iou(preds, targets)\n",
    "\n",
    "        # Jaccard Loss for Positive and Negative IoU\n",
    "        jaccard_loss_positive = 1.0 - iou_positive  # Positive IoU\n",
    "        jaccard_loss_negative = 1.0 - (2 * mean_iou - iou_positive)  # Derive Negative IoU\n",
    "\n",
    "        # Weighted Jaccard Loss\n",
    "        jaccard_loss = 0.85 * jaccard_loss_positive + 0.15 * jaccard_loss_negative\n",
    "\n",
    "        # Combine all losses\n",
    "        total_loss = 0.3 * bce_loss + 0.3 * focal_loss + 0.4 * jaccard_loss\n",
    "        return total_loss\n",
    "\n",
    "#loss = BCEFocalJaccardLoss(alpha=0.8, gamma=2)\n",
    "#loss = BCEFocalJaccardIoULoss(alpha=0.8, gamma=1.5)\n",
    "loss = BCEFocalNegativeIoULoss(alpha=0.8, gamma=2)\n",
    "loss_fn = loss\n",
    "\n",
    "    \n",
    "# optimizer = torch.optim.Adam([ \n",
    "#     dict(params=model.parameters(), lr=0.0006),\n",
    "#     #dict(params=model.parameters(), lr=0.00001),\n",
    "# ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    #lr=0.0006,               \n",
    "    lr=0.0001,               \n",
    "    weight_decay=1e-6        \n",
    "    #weight_decay=0        \n",
    ")\n",
    "\n",
    "# Initialize the Global Threshold to predict sigmoid inputs\n",
    "g_threshold = 0.5\n",
    "\n",
    "\n",
    "# Adjusts every 'step_size' epochs, decreasing by 'gamma'*100 %\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# add Warmup to the scheduler\n",
    "# num_epochs = 400\n",
    "# num_warmup_steps = 50\n",
    "\n",
    "# scheduler = get_cosine_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=num_warmup_steps,\n",
    "#     num_training_steps=num_epochs * len(train_loader)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### substitute SMP library to handle the trainig, validation and testing original loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)\n",
    "# valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=40, shuffle=False, num_workers=0)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=30, shuffle=False, num_workers=0)\n",
    "\n",
    "# new batch sizes for super resolution\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=5, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Device setup\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move the model to GPU\n",
    "model = model.to(DEVICE)  # This moves your model to the GPU (or keeps it on CPU if no GPU)\n",
    "\n",
    "def compute_class_aware_iou(preds, masks, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute IoU for positive and negative classes with a dynamic threshold.\n",
    "    Args:\n",
    "        preds: Predicted probabilities (logits or after sigmoid).\n",
    "        masks: Ground truth masks.\n",
    "        threshold: Threshold to classify predictions (default is 0.3).\n",
    "\n",
    "    Returns:\n",
    "        iou_positive: IoU for the positive class.\n",
    "        mean_iou: Mean IoU across both classes.\n",
    "    \"\"\"\n",
    "    # Apply threshold to predictions\n",
    "    preds = (preds > threshold).int()\n",
    "    masks = masks.int()\n",
    "\n",
    "    preds_np = preds.cpu().numpy().reshape(-1)\n",
    "    masks_np = masks.cpu().numpy().reshape(-1)\n",
    "\n",
    "    # Compute IoU for positive and negative classes\n",
    "    iou_positive = jaccard_score(masks_np, preds_np, pos_label=1)\n",
    "    iou_negative = jaccard_score(masks_np, preds_np, pos_label=0)\n",
    "\n",
    "    # Mean IoU\n",
    "    mean_iou = (iou_positive + iou_negative) / 2\n",
    "    return iou_positive, mean_iou\n",
    "\n",
    "\n",
    "\n",
    "# Define training loop function\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, discard_allbkgnd = True):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_iou = 0\n",
    "    num_batches = 0\n",
    "    iou_metric = JaccardIndex(task='binary', threshold=0.5).to(device)\n",
    "    nb_blank = 0\n",
    "    nb_tot_img = 0\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Check which images in the batch have all-background masks\n",
    "        valid_mask_indices = [i for i in range(masks.size(0)) if masks[i].sum() > 0]\n",
    "        \n",
    "        # number of all-background images per batch\n",
    "        nb_blank += masks.size(0) - len(valid_mask_indices)\n",
    "        nb_tot_img += masks.size(0)\n",
    "\n",
    "        # Control all background images\n",
    "        if discard_allbkgnd == False:\n",
    "            valid_mask_indices = range(masks.size(0))\n",
    "            \n",
    "        # Skip this batch if no valid masks\n",
    "        if len(valid_mask_indices) == 0:\n",
    "            print(\"Skipped batch with all all-background images.\")\n",
    "            continue  \n",
    "\n",
    "        # Filter images and masks for valid entries\n",
    "        valid_images = images[valid_mask_indices]\n",
    "        valid_masks = masks[valid_mask_indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with autocast():\n",
    "            preds = model(valid_images)\n",
    "            loss = loss_fn(preds, masks)\n",
    "\n",
    "        # Forward pass\n",
    "        # preds = model(valid_images)\n",
    "        # loss = loss_fn(preds, valid_masks)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Scale loss and perform backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        # Calculate IoU for valid images\n",
    "        total_iou += iou_metric(preds, valid_masks).item() * len(valid_mask_indices)\n",
    "        num_batches += len(valid_mask_indices)\n",
    "\n",
    "    # Avoid division by zero if all batches are skipped\n",
    "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_iou = total_iou / num_batches if num_batches > 0 else 0\n",
    "    print(f\"Discard 0s? {discard_allbkgnd}\\t| 0s_masks/Tot_imgs: {nb_blank}/{nb_tot_img} \\t|| Train Loss: {avg_loss:.6f} | Train IoU: {avg_iou:.3f}\")\n",
    "    return avg_loss, avg_iou\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, loss_fn, device, threshold=0.5, discard_allbkgnd=True):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_iou = 0.0\n",
    "    num_valid_images = 0\n",
    "    iou_metric = JaccardIndex(task='binary', threshold=threshold).to(device)\n",
    "    nb_blank = 0\n",
    "    nb_tot_img = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Check which images in the batch have valid masks\n",
    "            valid_mask_indices = [i for i in range(masks.size(0)) if masks[i].sum() > 0]\n",
    "\n",
    "            # number of all-background images per batch\n",
    "            nb_blank += masks.size(0) - len(valid_mask_indices)\n",
    "            nb_tot_img += masks.size(0)\n",
    "\n",
    "            # Control all background images\n",
    "            if discard_allbkgnd == False:\n",
    "                valid_mask_indices = range(masks.size(0))\n",
    "                \n",
    "            # Skip this batch if no valid masks\n",
    "            if len(valid_mask_indices) == 0:\n",
    "                print(\"Skipped batch with all all-background images.\")\n",
    "                continue  \n",
    "            \n",
    "            # Filter images and masks for valid entries\n",
    "            valid_images = images[valid_mask_indices]\n",
    "            valid_masks = masks[valid_mask_indices]\n",
    "\n",
    "            preds = model(valid_images)\n",
    "            loss = loss_fn(preds, valid_masks)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Threshold predictions for metric computation\n",
    "            preds = (preds > threshold).int()\n",
    "\n",
    "            # Calculate IoU for valid images\n",
    "            total_iou += iou_metric(preds, valid_masks).item() * len(valid_mask_indices)\n",
    "            num_valid_images += len(valid_mask_indices)\n",
    "\n",
    "    avg_loss = epoch_loss / num_valid_images if num_valid_images > 0 else 0\n",
    "    avg_iou = total_iou / num_valid_images if num_valid_images > 0 else 0\n",
    "    #print(f\"0s_masks/Tot_imgs: {nb_blank}/{nb_tot_img} \\t|| Valid Loss: {avg_loss:.6f} | Valid IoU: {avg_iou:.3f}\")\n",
    "    print(f\"Discard 0s? {discard_allbkgnd}\\t| 0s_masks/Tot_imgs: {nb_blank}/{nb_tot_img} \\t|| Valid Loss: {avg_loss:.6f} | Valid IoU: {avg_iou:.3f}\")\n",
    "    return avg_loss, avg_iou\n",
    "    \n",
    "\n",
    "# Define test loop function\n",
    "def test_one_epoch(model, dataloader, loss_fn, device, threshold=0.5, discard_allbkgnd = True):\n",
    "    \"\"\"\n",
    "    Test the model for one epoch with a dynamic threshold.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_iou = 0.0\n",
    "    num_valid_images = 0\n",
    "    #num_batches = 0\n",
    "    iou_metric = JaccardIndex(task='binary', threshold=threshold).to(device)\n",
    "    nb_blank = 0\n",
    "    nb_tot_img = 0\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "                \n",
    "            # debugging\n",
    "            #print(f' deb: {j} masks.size(0) {masks.size(0)}')\n",
    "            #print(f' deb: {j} images.size(0) {masks.size(0)}')\n",
    "            \n",
    "            valid_mask_indices = [i for i in range(masks.size(0)) if masks[i].sum() > 0]\n",
    "            #print(f' deb: {j} len(valid_mask_indices) {len(valid_mask_indices)}')\n",
    "\n",
    "            # number of all-background images per batch\n",
    "            nb_blank += masks.size(0) - len(valid_mask_indices)\n",
    "            nb_tot_img += masks.size(0)\n",
    "\n",
    "            # Control all background images\n",
    "            if discard_allbkgnd == False:\n",
    "                valid_mask_indices = range(masks.size(0))\n",
    "            \n",
    "            # Skip this batch if no valid masks\n",
    "            if len(valid_mask_indices) == 0:\n",
    "                print(\"Skipped batch with all all-background images.\")\n",
    "                continue  \n",
    "            \n",
    "            # Filter images and masks for valid entries\n",
    "            valid_images = images[valid_mask_indices]\n",
    "            valid_masks = masks[valid_mask_indices]\n",
    "\n",
    "            preds = model(valid_images)\n",
    "            loss = loss_fn(preds, valid_masks)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Threshold predictions\n",
    "            preds = (preds > threshold).int()\n",
    "            \n",
    "            # Calculate IoU for valid images\n",
    "            total_iou += iou_metric(preds, valid_masks).item() * len(valid_mask_indices)\n",
    "            num_valid_images += len(valid_mask_indices)\n",
    "            \n",
    "    avg_loss = epoch_loss / num_valid_images if num_valid_images > 0 else 0\n",
    "    avg_iou = total_iou / num_valid_images if num_valid_images > 0 else 0\n",
    "    #print(f\"0s_masks/Tot_imgs: {nb_blank}/{nb_tot_img} \\t|| Test Loss:  {avg_loss:.6f} | Test IoU: {avg_iou:.3f}\")\n",
    "    print(f\"Discard 0s? {discard_allbkgnd}\\t| 0s_masks/Tot_imgs: {nb_blank}/{nb_tot_img} \\t|| Test Loss: {avg_loss:.6f} | Test IoU: {avg_iou:.3f}\")\n",
    "    return avg_loss, avg_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement before stopping.\n",
    "            min_delta (float): Minimum change in the monitored metric to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = np.inf  # Start with infinity for minimizing metrics (e.g., validation loss)\n",
    "        self.counter = 0  # Count epochs without improvement\n",
    "\n",
    "    def __call__(self, current_metric):\n",
    "        # Check if the current metric is better than the best metric\n",
    "        if current_metric < self.best_metric - self.min_delta:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0  # Reset counter if there is an improvement\n",
    "            return False  # Do not stop, continue training\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                return True  # Stop training\n",
    "            return False  # Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-08 15:06:45] Preloading images and masks into GPU memory...\n",
      "[2024-12-08 15:06:45] Preloading complete.\n",
      "[2024-12-08 15:06:45] Preloading images and masks into GPU memory...\n",
      "[2024-12-08 15:06:45] Preloading complete.\n",
      "[2024-12-08 15:06:45] Preloading images and masks into GPU memory...\n",
      "[2024-12-08 15:06:45] Preloading complete.\n",
      "\n",
      "Starting training loop...\n",
      "Training 400 epochs...\n",
      "\n",
      "Epoch 1/400:\n",
      "[2024-12-08 15:06:46] Loading image and mask for index 115...\n",
      "Loading image: ../5-Data_Wrangling/data_split2sr/train/input\\img(163).tif\n",
      "[2024-12-08 15:06:46] Error at index 115: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fpere\\AppData\\Local\\Temp\\ipykernel_6384\\83172756.py:48: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 230\u001b[0m\n\u001b[0;32m    228\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Call the main function\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[71], line 125\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m    122\u001b[0m epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m train_loss, train_iou_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m valid_loss, valid_iou_score \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, valid_loader, loss_fn, DEVICE)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed. Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_iou_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 50\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, loss_fn, device, discard_allbkgnd)\u001b[0m\n\u001b[0;32m     46\u001b[0m nb_tot_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     48\u001b[0m scaler \u001b[38;5;241m=\u001b[39m GradScaler()\n\u001b[1;32m---> 50\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Check which images in the batch have all-background masks\u001b[39;49;00m\n",
      "File \u001b[1;32mE:\\Devs\\pyEnv-1\\UCSD_MLBootcamp_Capstone\\5b-Run_a_Model\\5b-Model-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mE:\\Devs\\pyEnv-1\\UCSD_MLBootcamp_Capstone\\5b-Run_a_Model\\5b-Model-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mE:\\Devs\\pyEnv-1\\UCSD_MLBootcamp_Capstone\\5b-Run_a_Model\\5b-Model-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[32], line 88\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Error at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[32], line 34\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Retrieve preloaded image and mask\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# image = self.images[index]\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# mask = self.masks[index]\u001b[39;00m\n\u001b[0;32m     32\u001b[0m  \u001b[38;5;66;03m# Debugging: Add print statements for image and mask paths\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading mask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index], cv2\u001b[38;5;241m.\u001b[39mIMREAD_UNCHANGED)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     36\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks[index], cv2\u001b[38;5;241m.\u001b[39mIMREAD_UNCHANGED)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "\n",
    "def main(num_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    test_losses = []\n",
    "    train_ious = []\n",
    "    valid_ious = []\n",
    "    test_ious = []\n",
    "\n",
    " \n",
    "    # Initialize dataset and DataLoader\n",
    "    train_dataset = CustomDataset(train_image_paths, train_mask_paths, transform=augmentation_transform, band=model_band)\n",
    "    val_dataset = CustomDataset(val_image_paths, val_mask_paths, transform=simple_transform, band=model_band)\n",
    "    test_dataset = CustomDataset(test_image_paths, test_mask_paths, transform=simple_transform, band=model_band)\n",
    "\n",
    "    # Wrap train_dataset with ProfileIterDataPipe for profiling\n",
    "    #profiled_train_dataset = ProfileIterDataPipe(train_dataset)\n",
    "\n",
    "    # print(\"[INFO] Profiling DataLoader performance...\")\n",
    "    # # Profile DataLoader\n",
    "    # print(\"[INFO] Profiling DataLoader performance...\")\n",
    "    # profiled_train_loader = DataLoader(\n",
    "    #     profiled_train_dataset,\n",
    "    #     batch_size=2,  # Use small batch size for debugging\n",
    "    #     shuffle=True,\n",
    "    #     num_workers=2,  # Change num_workers to fit your CPU cores\n",
    "    #     pin_memory=torch.cuda.is_available()\n",
    "    # )\n",
    "\n",
    "    # # Process a few batches to profile\n",
    "    # for i, (images, masks) in enumerate(profiled_train_loader):\n",
    "    #     print(f\"Profiling Batch {i + 1}:\")\n",
    "    #     print(f\" - Image tensor shape: {images.shape}, dtype: {images.dtype}\")\n",
    "    #     print(f\" - Mask tensor shape: {masks.shape}, dtype: {masks.dtype}\")\n",
    "    #     if i >= 4:  # Profile first 5 batches only\n",
    "    #         break\n",
    "\n",
    "    # Reinitialize train_loader after profiling\n",
    "    # print(\"[INFO] Reverting to the standard DataLoader for training...\")\n",
    "    # train_loader = DataLoader(\n",
    "    #     train_dataset,\n",
    "    #     batch_size=2,  # Original batch size for training\n",
    "    #     shuffle=True,\n",
    "    #     num_workers=0,  # Same workers as above\n",
    "    #     pin_memory=torch.cuda.is_available()\n",
    "    # )\n",
    "    \n",
    "    # Batch sizes for super resolution\n",
    "    #train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    # valid_loader = torch.utils.data.DataLoader(\n",
    "    #     val_dataset, \n",
    "    #     batch_size=2, \n",
    "    #     shuffle=False, \n",
    "    #     num_workers=0, \n",
    "    #     pin_memory=True)\n",
    "    \n",
    "    # test_loader = torch.utils.data.DataLoader(\n",
    "    #     test_dataset, \n",
    "    #     batch_size=2, \n",
    "    #     shuffle=False, \n",
    "    #     num_workers=0, \n",
    "    #     pin_memory=True)\n",
    "\n",
    "    # Define the model, loss function, optimizer, and scheduler\n",
    "    model = smp.UnetPlusPlus(\n",
    "        in_channels=4 if use_all_bands else 1,\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights=ENCODER_WEIGHTS,\n",
    "        classes=len(CLASSES),\n",
    "        activation=ACTIVATION\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    loss_fn = BCEFocalNegativeIoULoss(alpha=0.8, gamma=2)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        weight_decay=1e-6\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
    "\n",
    "    \n",
    "    # Force to include the test set loss and score metric into the training loop\n",
    "    include_test = False\n",
    "\n",
    "    # Save the model's parameters\n",
    "    model_filename = './models/unetpp_effb7.pth'\n",
    "    \n",
    "    max_score = float('inf')  # Initialize with a high value to store the best validation loss\n",
    "    \n",
    "    # Initialize early stopping\n",
    "    #early_stopping = EarlyStopping(patience=20, min_delta=0.00001)\n",
    "   \n",
    "    tic = time.time()\n",
    "    print(\"\\nStarting training loop...\")\n",
    "    \n",
    "    # Ensure loss_fn is defined globally or passed correctly\n",
    "    #global loss_fn  # Add this line if loss_fn is already defined globally\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Training {num_epochs} epochs...\")\n",
    "    total_train_time = 0\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA\n",
    "        ],\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True\n",
    "    ) as prof:\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}:\")\n",
    "            \n",
    "            train_loss, train_iou_score = train_one_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
    "            valid_loss, valid_iou_score = validate_one_epoch(model, valid_loader, loss_fn, DEVICE)\n",
    "    \n",
    "            print(f\"Epoch {epoch + 1} completed. Train Loss: {train_loss:.4f}, Train IoU: {train_iou_score:.4f}\")\n",
    "            print(f\"Validation Loss: {valid_loss:.4f}, Validation IoU: {valid_iou_score:.4f}\")\n",
    "    \n",
    "            # Update scheduler\n",
    "            scheduler.step(valid_loss)\n",
    "    \n",
    "            # Save the best model\n",
    "            if valid_loss < scheduler.best:\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "                print(\"Model saved.\")\n",
    "    \n",
    "            # Check early stopping\n",
    "            if early_stopping(valid_loss):\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "            total_train_time += time.time() - epoch_start\n",
    "            \n",
    "        # for epoch in range(num_epochs):\n",
    "        #     print(f'\\nEpoch: {epoch}')\n",
    "            \n",
    "        #     epoch_start = time.time()\n",
    "            \n",
    "        #     # Training\n",
    "        #     model.train()\n",
    "        #     total_train_time = 0\n",
    "        #     for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        #         batch_start = time.time()\n",
    "                \n",
    "        #         # Data to GPU\n",
    "        #         data_transfer_start = time.time()\n",
    "        #         images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        #         data_transfer_time = time.time() - data_transfer_start\n",
    "                \n",
    "        #         # Forward pass\n",
    "        #         forward_start = time.time()\n",
    "        #         outputs = model(images)\n",
    "        #         forward_time = time.time() - forward_start\n",
    "                \n",
    "        #         # Loss computation\n",
    "        #         loss_start = time.time()\n",
    "        #         loss = loss_fn(outputs, masks)\n",
    "        #         loss_time = time.time() - loss_start\n",
    "                \n",
    "        #         # Backward pass and optimization\n",
    "        #         backward_start = time.time()\n",
    "        #         optimizer.zero_grad()\n",
    "        #         loss.backward()\n",
    "        #         optimizer.step()\n",
    "        #         backward_time = time.time() - backward_start\n",
    "                \n",
    "        #         batch_time = time.time() - batch_start\n",
    "        #         total_train_time += batch_time\n",
    "    \n",
    "        #         print(f\"Batch {batch_idx}: Data transfer {data_transfer_time:.4f}s, \"\n",
    "        #               f\"Forward {forward_time:.4f}s, Loss {loss_time:.4f}s, Backward {backward_time:.4f}s, \"\n",
    "        #               f\"Total {batch_time:.4f}s\")\n",
    "            \n",
    "            train_loss, train_iou_score = train_one_epoch(model, train_loader, optimizer, loss_fn=loss_fn, device=DEVICE, discard_allbkgnd=True)\n",
    "            train_losses.append(train_loss)\n",
    "            train_ious.append(train_iou_score)\n",
    "            \n",
    "            # Validation\n",
    "            valid_loss, valid_iou_score = validate_one_epoch(model, valid_loader, loss_fn=loss_fn, device=DEVICE, discard_allbkgnd=True)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_ious.append(valid_iou_score)\n",
    "        \n",
    "            # # Check early stopping criteria with the validation loss\n",
    "            # if early_stopping(valid_loss):\n",
    "            #     print(f\"Stopping at epoch {epoch}\")\n",
    "            #     break  \n",
    "    \n",
    "            # if include_test:\n",
    "            #     # Run testing\n",
    "            #     test_loss, test_iou_score = test_one_epoch(model, test_loader, loss_fn=loss_fn, device=DEVICE, discard_allbkgnd=True)\n",
    "            #     test_losses.append(test_loss)\n",
    "            #     test_ious.append(test_iou_score)\n",
    "            \n",
    "            # # Update the learning rate\n",
    "            # scheduler.step(valid_loss)\n",
    "            \n",
    "            # Save the model if validation loss improves\n",
    "            if valid_loss < max_score:\n",
    "                max_score = valid_loss\n",
    "                torch.save(model, model_filename)\n",
    "                print('Model saved!')\n",
    "            \n",
    "            # epoch_time = time.time() - epoch_start\n",
    "            # print(f\"Epoch {epoch} completed in {epoch_time:.2f} seconds. Total training time: {total_train_time:.2f} seconds.\")\n",
    "    \n",
    "            print(f\"Total training time: {total_train_time / 60:.2f} minutes.\")\n",
    "\n",
    "    # toc = time.time()\n",
    "    # elapsed_time = toc - tic\n",
    "    # print(f\"\\nTraining completed in {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the start method to spawn\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    num_epochs = 400\n",
    "    # Call the main function\n",
    "    main(num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot metrics after training\n",
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "# Plot training/validation losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.title(\"Losses over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot IoU for validation and test sets\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_ious, label=\"Training IoU\")\n",
    "plt.plot(valid_ious, label=\"Validation IoU\")\n",
    "plt.plot(test_ious, label=\"Test IoU\")\n",
    "plt.title(\"IoU over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DLV3\n",
    "#model = torch.load('datasets/solar_panel/models_new/test.pth')\n",
    "#model = torch.load('./models/dlv3_effb0.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/dlv3_r101.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/dlv3_r50.pth')\n",
    "\n",
    "# UNET\n",
    "#model = torch.load('datasets/solar_panel/models_new/unet_r101.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/unet_effb0.pth')\n",
    "##model = torch.load('./models/unet_effb7.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/unet_r50.pth')\n",
    "\n",
    "# UNET++\n",
    "model = torch.load('./models/unetpp_effb7.pth')\n",
    "\n",
    "#PSPNet\n",
    "#model = torch.load('datasets/solar_panel/models_new/psp_effb7_at.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/psp_effb0.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/psp_r101.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/psp_r50.pth')\n",
    "\n",
    "# FPN\n",
    "#model = torch.load('datasets/solar_panel/models_new/fpn_effb7.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/fpn_effb0.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/fpn_r101.pth')\n",
    "#model = torch.load('datasets/solar_panel/models_new/fpn_r50.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "# Ensure no gradients are calculated, as this is evaluation\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Fetch a batch from the test loader\n",
    "    inp, lab = next(iter(test_loader))\n",
    "    inp = inp.to(DEVICE)\n",
    "    lab = lab.to(DEVICE)\n",
    "\n",
    "    # Predict\n",
    "    pred = model(inp)\n",
    "    #pred = (pred.sigmoid() > 0.5).float()\n",
    "    #pred = pred.sigmoid()  \n",
    "\n",
    "    # Reshape if necessary to match [batch_size, 1, height, width]\n",
    "    if pred.shape[1] != 1:\n",
    "        pred = pred.unsqueeze(1)\n",
    "\n",
    "    if lab.shape[1] != 1:\n",
    "        lab = lab.unsqueeze(1)\n",
    "\n",
    "    # Calculate IoU score for the batch\n",
    "    iou_metric = JaccardIndex(task='binary', threshold=0.5).to(DEVICE)  # Use binary IoU for evaluation\n",
    "    iou_score = iou_metric(pred, lab)\n",
    "\n",
    "    print(f\"Sample IoU Score: {iou_score.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, IoU, and F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, jaccard_score, f1_score\n",
    "\n",
    "# # Flatten the labels and predictions for metric computation\n",
    "# lbl = lab.cpu().numpy().reshape(-1)  # Convert the ground truth mask to a 1D array\n",
    "# target = pred.cpu().numpy().reshape(-1)  # Convert the predicted mask to a 1D array\n",
    "\n",
    "# # Check value distributions in lbl and target\n",
    "# #print(f\"Label distribution: {np.unique(lbl, return_counts=True)}\")\n",
    "# #print(f\"Prediction distribution: {np.unique(target, return_counts=True)}\")\n",
    "\n",
    "# # Convert predictions to binary format at two different thresholds\n",
    "\n",
    "# target_25 = np.where(target > 0.25, 1, 0)\n",
    "# target_50 = np.where(target > 0.5, 1, 0)\n",
    "# target_75 = np.where(target > 0.75, 1, 0)\n",
    "\n",
    "# # Ensure ground truth is binary\n",
    "# lbl = np.where(lbl > 0.5, 1, 0)\n",
    "\n",
    "# def compute_metrics(label, target_25, target_50, target_75):\n",
    "#     # Compute accuracy at different thresholds\n",
    "#     acc_25 = accuracy_score(label, target_25)\n",
    "#     acc_50 = accuracy_score(label, target_50)\n",
    "#     acc_75 = accuracy_score(label, target_75)\n",
    "    \n",
    "#     # Compute IoU (Jaccard score) at different thresholds\n",
    "#     iou_25 = jaccard_score(label, target_25)\n",
    "#     iou_50 = jaccard_score(label, target_50)\n",
    "#     iou_75 = jaccard_score(label, target_75)\n",
    "    \n",
    "#     # Compute F1-score at different thresholds\n",
    "#     f_25 = f1_score(label, target_25)\n",
    "#     f_50 = f1_score(label, target_50)\n",
    "#     f_75 = f1_score(label, target_75)\n",
    "    \n",
    "#     # Display results\n",
    "#     print('Thresholds: \\t 25% \\t| 50% \\t| 75%')\n",
    "#     print(40*'-')\n",
    "#     print('Accuracy:\\t', round(acc_25 * 100, 2), '|', round(acc_50 * 100, 2), '|', round(acc_75 * 100, 2))\n",
    "#     print('IoU:\\t\\t', round(iou_25 * 100, 2), '|', round(iou_50 * 100, 2), '|', round(iou_75 * 100, 2))\n",
    "#     print('F-score:\\t', round(f_25 * 100, 2), '|', round(f_50 * 100, 2), '|', round(f_75 * 100, 2))\n",
    "\n",
    "# # Compute and display the metrics\n",
    "# compute_metrics(lbl, target_25, target_50, target_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# def compute_confusion_matrix(model, test_loader, device, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Compute the confusion matrix for predictions with a dynamic threshold.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_masks = []\n",
    "\n",
    "#     nb_of_images_eval = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, masks in test_loader:\n",
    "            \n",
    "#             # Skip blank masks\n",
    "#             if masks.sum() == 0:\n",
    "#                 continue  \n",
    "                \n",
    "#             images = images.to(device)\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             preds = model(images)\n",
    "\n",
    "#             # Threshold predictions\n",
    "#             preds = (preds > threshold).int()\n",
    "\n",
    "#             all_preds.append(preds.cpu().numpy().ravel())\n",
    "#             all_masks.append(masks.cpu().numpy().ravel())\n",
    "\n",
    "#             nb_of_images_eval += 1\n",
    "\n",
    "#     all_preds = np.concatenate(all_preds)\n",
    "#     all_masks = np.concatenate(all_masks)\n",
    "\n",
    "#     # Compute confusion matrix\n",
    "#     cm = confusion_matrix(all_masks, all_preds, labels=[0, 1])\n",
    "#     return cm, nb_of_images_eval\n",
    "\n",
    "\n",
    "# def display_traditional_confusion_matrix_with_metrics(cm, nb_of_images):\n",
    "#     \"\"\"\n",
    "#     Display the confusion matrix with the traditional layout and print additional metrics.\n",
    "#     Args:\n",
    "#         cm: Confusion matrix (2x2 for binary classification).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # New labels for the classes\n",
    "#     labels = [\"Background\", \"PV\"]\n",
    "\n",
    "#     # Extract values from confusion matrix\n",
    "#     TN = cm[0, 0]  # True Negative\n",
    "#     FP = cm[0, 1]  # False Positive\n",
    "#     FN = cm[1, 0]  # False Negative\n",
    "#     TP = cm[1, 1]  # True Positive\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "#     recall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensitivity (Recall)\n",
    "#     specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # True Negative Rate\n",
    "#     precision = TP / (TP + FP) if (TP + FP) > 0 else 0  # Positive Predictive Value\n",
    "#     f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "#     IoU = TP / (TP + FN + FP) if (TP + FN + FP) > 0 else 0  # IoU\n",
    "\n",
    "#     # Print metrics\n",
    "#     print(f\"\\nNb of images computed: {nb_of_images}\")\n",
    "#     print(f\"TP:{TP}\\tTN:{TN}\\tFP:{FP}\\t\\tFN:{FN}\")\n",
    "#     print(70*'-')\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"Accuracy:     {accuracy:.4f}\\t               (TP + TN) / (TP + TN + FP + FN)\")\n",
    "#     print(f\"Recall:       {recall:.4f}\\t                      TP / (TP + FN)\")\n",
    "#     print(f\"Specificity:  {specificity:.4f}\\t                      TN / (TN + FP)\")\n",
    "#     print(f\"Precision:    {precision:.4f}\\t                      TP / (TP + FP)\")\n",
    "#     print(f\"F1-Score:     {f1_score:.4f}\\t    (2*precision*recall) / (precision + recall)\")\n",
    "#     print(f\"IoU:          {IoU:.4f}\\t                      TP / (TP + FN + FP)\")\n",
    "#     print(\"\\n\\n\")\n",
    "    \n",
    "#     # Display the confusion matrix\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "#     disp.plot(cmap=plt.cm.Blues)\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "#     plt.xlabel(\"Predicted label\")\n",
    "#     plt.ylabel(\"True label\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# cm, nb_of_images = compute_confusion_matrix(model, test_loader, DEVICE)\n",
    "# display_traditional_confusion_matrix_with_metrics(cm, nb_of_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOW PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import jaccard_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# Ensure the output directory is cleared and exists\n",
    "output_dir = \"imgs\"\n",
    "\n",
    "# Clear the output directory if it exists\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)  # Delete all files and the directory\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create a fresh directory\n",
    "\n",
    "\n",
    "actual_img_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    for data in test_loader:\n",
    "        inp, lab = data\n",
    "\n",
    "        inp = inp.to(DEVICE)\n",
    "        lab = lab.to(DEVICE)\n",
    "        pred = model(inp) \n",
    "\n",
    "        #print(len(inp))\n",
    "        \n",
    "        # Loop through the batch\n",
    "        for i in range(len(inp)):\n",
    "            iou_score = 0\n",
    "            \n",
    "            # Extract and process the ground truth and input image\n",
    "            lab_unit = lab[i, 0, :, :].cpu().numpy()  # Take single channel for ground truth\n",
    "            inp_unit = inp[i].cpu().numpy()  # Extract input and move to CPU\n",
    "            inp_unit = np.transpose(inp_unit[:3], (1, 2, 0))  # Take RGB channels, adjust to HxWxC\n",
    "            \n",
    "            # Process prediction and calculate IoU score\n",
    "            pred_img = pred[i, 0, :, :].cpu().numpy()  # Single channel prediction\n",
    "            #pred_img = np.where(pred_img > 0.5, 1, 0)  # Threshold to binary\n",
    "            pred_img = np.where(pred_img > g_threshold, 1, 0)\n",
    "            \n",
    "            iou_score = jaccard_score(lab_unit.reshape(-1), pred_img.reshape(-1), zero_division=0)\n",
    "\n",
    "            # Plot images\n",
    "            NUM_ROWS = 1\n",
    "            IMGs_IN_ROW = 3\n",
    "            f, ax = plt.subplots(NUM_ROWS, IMGs_IN_ROW, figsize=(10, 10))\n",
    "\n",
    "            #ax[0].imshow(inp_unit / 3000)  # Adjust scale as needed for input display\n",
    "            ax[0].imshow((inp_unit - inp_unit.min()) / (inp_unit.max() - inp_unit.min()))  # Normalize for display\n",
    "            ax[1].imshow(pred_img, cmap='binary_r')\n",
    "            ax[2].imshow(lab_unit, cmap='binary_r')\n",
    "\n",
    "            ax[0].set_title(f'Original Image | {actual_img_count + 1}')\n",
    "            ax[1].set_title(f'Prediction | {actual_img_count + 1}')\n",
    "            ax[2].set_title(f'Ground Truth | {actual_img_count + 1}')\n",
    "\n",
    "            # Remove tick labels for a cleaner plot\n",
    "            for a in ax:\n",
    "                a.set_yticklabels([])\n",
    "                a.set_xticklabels([])\n",
    "\n",
    "            print(f'ID: {actual_img_count + 1} | IoU: {round(iou_score, 3)}')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save selected images\n",
    "            if actual_img_count in [15, 22, 29]:\n",
    "                plt.savefig(f'imgs/img{actual_img_count}.png', dpi=500, bbox_inches='tight', pad_inches=0)\n",
    "            plt.show()\n",
    "            \n",
    "            actual_img_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# def compute_confusion_matrix_with_distributions(model, test_loader, device):\n",
    "#     \"\"\"\n",
    "#     Compute the confusion matrix for all pixels in the test set.\n",
    "#     Args:\n",
    "#         model: Trained segmentation model.\n",
    "#         test_loader: DataLoader for the test set.\n",
    "#         device: Device ('cuda' or 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#         cm: Confusion matrix (2x2 for binary classification).\n",
    "#     \"\"\"\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     all_preds = []\n",
    "#     all_masks = []\n",
    "\n",
    "#     nb_of_images_eval = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, masks in test_loader:\n",
    "#             # Ensure we're using only test images\n",
    "#             assert len(images) > 0, \"No images found in test_loader.\"\n",
    "            \n",
    "#             images = images.to(device)\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             # Get predictions\n",
    "#             preds = model(images)\n",
    "\n",
    "#             # Ensure predictions and masks have the same spatial dimensions\n",
    "#             assert preds.shape == masks.shape, f\"Shape mismatch: preds {preds.shape}, masks {masks.shape}\"\n",
    "            \n",
    "#             # Flatten predictions and masks for confusion matrix computation\n",
    "#             all_preds.append(preds.cpu().numpy().ravel())\n",
    "#             all_masks.append(masks.cpu().numpy().ravel())\n",
    "\n",
    "#             nb_of_images_eval += 1\n",
    "\n",
    "#     # Concatenate all predictions and masks\n",
    "#     all_preds = np.concatenate(all_preds)\n",
    "#     all_masks = np.concatenate(all_masks)\n",
    "\n",
    "#     # Debug: Check final shapes\n",
    "#     print(f\"Final shapes - all_preds: {all_preds.shape}, all_masks: {all_masks.shape}\")\n",
    "#     assert len(all_preds) == len(all_masks), f\"Length mismatch: preds {len(all_preds)}, masks {len(all_masks)}\"\n",
    "\n",
    "#     # Compute confusion matrix\n",
    "#     binary_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "#     cm = confusion_matrix(all_masks, binary_preds, labels=[0, 1])\n",
    " \n",
    "#     # Separate probabilities for positive and negative classes\n",
    "#     pos_probs = all_preds[all_masks == 1]  # Positive class (PV)\n",
    "#     neg_probs = all_preds[all_masks == 0]  # Negative class (Background)\n",
    "\n",
    "#     return cm, pos_probs, neg_probs, nb_of_images_eval\n",
    "\n",
    "\n",
    "def compute_confusion_matrix_with_distributions(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix for all pixels in the test set across all batches.\n",
    "    Args:\n",
    "        model: Trained segmentation model.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        device: Device ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        cm: Confusion matrix (2x2 for binary classification).\n",
    "        pos_probs: Predicted probabilities for the positive class (PV).\n",
    "        neg_probs: Predicted probabilities for the negative class (Background).\n",
    "        nb_of_images_eval: Total number of images evaluated.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_masks = []\n",
    "\n",
    "    nb_of_images_eval = 0\n",
    "\n",
    "    # Debug: Check the dataset size\n",
    "    print(f\"Test loader dataset size: {len(test_loader.dataset)}\")\n",
    "    print(f\"Batch size: {test_loader.batch_size}, Total batches: {len(test_loader)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(test_loader):\n",
    "            # Ensure the batch contains images\n",
    "            if len(images) == 0:\n",
    "                print(f\"Skipped empty batch {batch_idx + 1}/{len(test_loader)}.\")\n",
    "                continue\n",
    "\n",
    "            # Debug: Print batch size\n",
    "            print(f\"Processing batch {batch_idx + 1}/{len(test_loader)}: {len(images)} images.\")\n",
    "\n",
    "            # Move images and masks to the device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = model(images)\n",
    "\n",
    "            # Ensure predictions and masks have consistent shapes\n",
    "            if preds.shape != masks.shape:\n",
    "                print(f\"Shape mismatch in batch {batch_idx + 1}: preds {preds.shape}, masks {masks.shape}\")\n",
    "                continue  # Skip batch with inconsistent shapes\n",
    "\n",
    "            # Debug: Print shapes of predictions and masks\n",
    "            print(f\"Batch {batch_idx + 1} shapes - preds: {preds.shape}, masks: {masks.shape}\")\n",
    "\n",
    "            # Flatten predictions and masks for confusion matrix computation\n",
    "            preds_flat = preds.cpu().numpy().ravel()\n",
    "            masks_flat = masks.cpu().numpy().ravel()\n",
    "\n",
    "            # Debug: Print flattened shapes\n",
    "            print(f\"Flattened batch {batch_idx + 1} shapes - preds: {preds_flat.shape}, masks: {masks_flat.shape}\")\n",
    "\n",
    "            # Append the flattened predictions and masks\n",
    "            all_preds.append(preds_flat)\n",
    "            all_masks.append(masks_flat)\n",
    "\n",
    "            nb_of_images_eval += len(images)\n",
    "\n",
    "    # Concatenate all predictions and masks across all batches\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_masks = np.concatenate(all_masks)\n",
    "\n",
    "    # Debug: Print final concatenated shapes\n",
    "    print(f\"Final concatenated shapes - all_preds: {all_preds.shape}, all_masks: {all_masks.shape}\")\n",
    "    assert len(all_preds) == len(all_masks), (\n",
    "        f\"Length mismatch after processing all batches: preds {len(all_preds)}, masks {len(all_masks)}\"\n",
    "    )\n",
    "\n",
    "    # Convert probabilities to binary predictions (threshold = 0.5)\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_masks, binary_preds, labels=[0, 1])\n",
    "\n",
    "    # Separate probabilities for positive and negative classes\n",
    "    pos_probs = all_preds[all_masks == 1]  # Positive class (PV)\n",
    "    neg_probs = all_preds[all_masks == 0]  # Negative class (Background)\n",
    "\n",
    "    return cm, pos_probs, neg_probs, nb_of_images_eval\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def display_confusion_matrix_with_metrics_and_distributions(cm, pos_probs, neg_probs, nb_of_images_eval):\n",
    "    \"\"\"\n",
    "    Display the confusion matrix with the traditional layout and print additional metrics.\n",
    "    Args:\n",
    "        cm: Confusion matrix (2x2 for binary classification).\n",
    "    \"\"\"\n",
    "    \n",
    "    # New labels for the classes\n",
    "    labels = [\"Background\", \"PV\"]\n",
    "\n",
    "    # Extract values from confusion matrix\n",
    "    TN = cm[0, 0]  # True Negative\n",
    "    FP = cm[0, 1]  # False Positive\n",
    "    FN = cm[1, 0]  # False Negative\n",
    "    TP = cm[1, 1]  # True Positive\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensitivity (Recall)\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0  # True Negative Rate\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0  # Positive Predictive Value\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    IoU = TP / (TP + FN + FP) if (TP + FN + FP) > 0 else 0  # IoU\n",
    "\n",
    "    # Print metrics\n",
    "    print(70*'-')\n",
    "    print(f\"TP:{TP}\\tTN:{TN}\\tFP:{FP}\\t\\tFN:{FN}\")\n",
    "    print(70*'-')\n",
    "    print(\"\\n\")\n",
    "    print(f\"Accuracy:     {accuracy:.4f}\\t               (TP + TN) / (TP + TN + FP + FN)\")\n",
    "    print(f\"Recall:       {recall:.4f}\\t                      TP / (TP + FN)\")\n",
    "    print(f\"Specificity:  {specificity:.4f}\\t                      TN / (TN + FP)\")\n",
    "    print(f\"Precision:    {precision:.4f}\\t                      TP / (TP + FP)\")\n",
    "    print(f\"F1-Score:     {f1_score:.4f}\\t    (2*precision*recall) / (precision + recall)\")\n",
    "    print(f\"IoU:          {IoU:.4f}\\t                      TP / (TP + FN + FP)\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # Display the confusion matrix\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=axes[0], colorbar=False)\n",
    "    axes[0].set_title(\"Confusion Matrix\")\n",
    "    axes[0].set_xlabel(\"Predicted label\")\n",
    "    axes[0].set_ylabel(\"True label\")\n",
    "\n",
    "    # Plot distributions of predicted probabilities\n",
    "    sns.kdeplot(neg_probs, fill=True, color=\"green\", alpha=0.5, label=\"Negative (Background)\", ax=axes[1])\n",
    "    sns.kdeplot(pos_probs, fill=True, color=\"blue\", alpha=0.5, label=\"Positive (PV)\", ax=axes[1])\n",
    "    \n",
    "    # Add a threshold line\n",
    "    axes[1].axvline(0.5, color=\"black\", linestyle=\"--\", label=\"Threshold\")\n",
    "    \n",
    "    # Set titles and labels\n",
    "    axes[1].set_title(\"Predicted Probability Distributions\")\n",
    "    axes[1].set_xlabel(\"Predicted Probability\")\n",
    "    axes[1].set_ylabel(\"Density\")\n",
    "    \n",
    "    # Add legend\n",
    "    axes[1].legend()\n",
    "\n",
    "\n",
    "# \n",
    "cm, pos_probs, neg_probs, nb_of_images_eval = compute_confusion_matrix_with_distributions(model, test_loader, DEVICE)\n",
    "display_confusion_matrix_with_metrics_and_distributions(cm, pos_probs, neg_probs, nb_of_images_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, pos_probs, neg_probs, nb_of_images_eval = compute_confusion_matrix_with_distributions(model, valid_loader, DEVICE)\n",
    "display_confusion_matrix_with_metrics_and_distributions(cm, pos_probs, neg_probs, nb_of_images_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_probability_distribution_zoom(pos_probs, neg_probs, zoom_ylim=(0, 5), threshold=0.5, bins=200):\n",
    "    \"\"\"\n",
    "    Plot the predicted probability distributions with a zoomed-in y-axis using histograms with more bins.\n",
    "    Args:\n",
    "        pos_probs: Predicted probabilities for the positive class (PV).\n",
    "        neg_probs: Predicted probabilities for the negative class (Background).\n",
    "        zoom_ylim: Tuple for y-axis limits (default is (0, 5)).\n",
    "        threshold: Threshold value to separate classes (default is 0.5).\n",
    "        bins: Number of bins for the histogram (default is 100).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Define bin edges to ensure alignment of histograms\n",
    "    bin_edges = np.linspace(0, 1, bins)\n",
    "\n",
    "    # Plot negative probabilities\n",
    "    plt.hist(neg_probs, bins=bin_edges, color=\"green\", alpha=0.5, label=\"Negative (Background)\", density=False)\n",
    "\n",
    "    # Plot positive probabilities\n",
    "    plt.hist(pos_probs, bins=bin_edges, color=\"blue\", alpha=0.5, label=\"Positive (PV)\", density=False)\n",
    "\n",
    "    # Add a threshold line\n",
    "    plt.axvline(threshold, color=\"black\", linestyle=\"--\", label=f\"Threshold ({threshold})\")\n",
    "\n",
    "    # Set titles and labels\n",
    "    plt.title(f\"Zoomed Predicted Probability Distributions with {bins} Bins\")\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.ylim(zoom_ylim)  # Adjust the y-axis limits to zoom in\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_probability_distribution_zoom(pos_probs, neg_probs, zoom_ylim=(0, 20000), bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score\n",
    "\n",
    "# Flatten the labels and predictions for metric computation\n",
    "lbl = lab.cpu().numpy().reshape(-1)  # Convert the ground truth mask to a 1D array\n",
    "target = pred.cpu().numpy().reshape(-1)  # Convert the predicted mask to a 1D array\n",
    "\n",
    "# Check value distributions in lbl and target\n",
    "#print(f\"Label distribution: {np.unique(lbl, return_counts=True)}\")\n",
    "#print(f\"Prediction distribution: {np.unique(target, return_counts=True)}\")\n",
    "\n",
    "# Convert predictions to binary format at two different thresholds\n",
    "\n",
    "\n",
    "target_1 = 0.45\n",
    "target_2 = 0.46\n",
    "target_3 = 0.47\n",
    "\n",
    "threshold_1 = np.where(target > target_1, 1, 0)\n",
    "threshold_2 = np.where(target > target_2, 1, 0)\n",
    "threshold_3 = np.where(target > target_3, 1, 0)\n",
    "\n",
    "# Ensure ground truth is binary\n",
    "lbl = np.where(lbl > 0.5, 1, 0)\n",
    "\n",
    "def compute_metrics(label, target_25, target_50, target_75):\n",
    "    # Compute accuracy at different thresholds\n",
    "    acc_25 = accuracy_score(label, target_25)\n",
    "    acc_50 = accuracy_score(label, target_50)\n",
    "    acc_75 = accuracy_score(label, target_75)\n",
    "    \n",
    "    # Compute IoU (Jaccard score) at different thresholds\n",
    "    iou_25 = jaccard_score(label, target_25)\n",
    "    iou_50 = jaccard_score(label, target_50)\n",
    "    iou_75 = jaccard_score(label, target_75)\n",
    "    \n",
    "    # Compute F1-score at different thresholds\n",
    "    f_25 = f1_score(label, target_25)\n",
    "    f_50 = f1_score(label, target_50)\n",
    "    f_75 = f1_score(label, target_75)\n",
    "    \n",
    "    # Display results\n",
    "    print(f'Thresholds: \\t {100*target_1:.2f}%\\t|{100*target_2:.2f}%\\t|{100*target_3:.2f}%')\n",
    "    print(40*'-')\n",
    "    print('Accuracy:\\t', round(acc_25 * 100, 2), '|', round(acc_50 * 100, 2), '|', round(acc_75 * 100, 2))\n",
    "    print('IoU:\\t\\t', round(iou_25 * 100, 2), '|', round(iou_50 * 100, 2), '|', round(iou_75 * 100, 2))\n",
    "    print('F-score:\\t', round(f_25 * 100, 2), '|', round(f_50 * 100, 2), '|', round(f_75 * 100, 2))\n",
    "\n",
    "# Compute and display the metrics\n",
    "compute_metrics(lbl, threshold_1, threshold_2, threshold_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probability_distribution_zoom(pos_probs, neg_probs, zoom_ylim=(0, 2), threshold=.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work on the pixels and understand FN and FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_img_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    data = next(iter(valid_loader))\n",
    "\n",
    "    inp, lab = data\n",
    "    \n",
    "    inp = inp.to(DEVICE)\n",
    "    lab = lab.to(DEVICE)\n",
    "    pred = model(inp) \n",
    "\n",
    "    i = 0\n",
    "    lab_unit = lab[i, 0, :, :].cpu().numpy()\n",
    "    inp_unit = inp[i].cpu().numpy()\n",
    "    inp_unit = np.transpose(inp_unit[:3], (1, 2, 0))\n",
    "\n",
    "    pred_img = pred[i, 0, :, :].cpu().numpy()\n",
    "    pred_img = np.where(pred_img > 0.5, 1, 0)  # Threshold to binary\n",
    "    \n",
    "    #print(pred_img.shape)\n",
    "    #print(pred_img[100,100])\n",
    "\n",
    "    #print(len(inp))\n",
    "\n",
    "    # Count positive and negative pixels in prediction and ground truth\n",
    "    pred_positive = np.sum(pred_img == 1)\n",
    "    pred_negative = np.sum(pred_img == 0)\n",
    "    lab_positive = np.sum(lab_unit == 1)\n",
    "    lab_negative = np.sum(lab_unit == 0)\n",
    "\n",
    "    # Print shapes and pixel counts\n",
    "    print(f\"Prediction Shape: {pred_img.shape}\")\n",
    "    print(f\"Prediction Positive Pixels (1): {pred_positive}\")\n",
    "    print(f\"Prediction Negative Pixels (0): {pred_negative}\")\n",
    "    print(f\"Ground Truth Positive Pixels (1): {lab_positive}\")\n",
    "    print(f\"Ground Truth Negative Pixels (0): {lab_negative}\")\n",
    "    #print(f\"Batch Size: {len(inp)}\")\n",
    "\n",
    "    # Calculate TP, TN, FP, FN\n",
    "    TP = np.sum((pred_img == 1) & (lab_unit == 1))  # True Positives\n",
    "    TN = np.sum((pred_img == 0) & (lab_unit == 0))  # True Negatives\n",
    "    FP = np.sum((pred_img == 1) & (lab_unit == 0))  # False Positives\n",
    "    FN = np.sum((pred_img == 0) & (lab_unit == 1))  # False Negatives\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensitivity\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    iou = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\\n\")\n",
    "    \n",
    "    print(\"Metrics:\")\n",
    "    print(f\"Accuracy: \\t\\t{accuracy:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): \\t{recall:.4f}\")\n",
    "    print(f\"Specificity: \\t\\t{specificity:.4f}\")\n",
    "    print(f\"Precision: \\t\\t{precision:.4f}\")\n",
    "    print(f\"F1-Score: \\t\\t{f1_score:.4f}\")\n",
    "    print(f\"IoU: \\t\\t\\t{iou:.4f}\")\n",
    "\n",
    "\n",
    "    # Plot images\n",
    "    NUM_ROWS = 1\n",
    "    IMGs_IN_ROW = 3\n",
    "    f, ax = plt.subplots(NUM_ROWS, IMGs_IN_ROW, figsize=(10, 10))\n",
    "\n",
    "    ax[0].imshow((inp_unit - inp_unit.min()) / (inp_unit.max() - inp_unit.min()))  # Normalize for display\n",
    "    ax[1].imshow(pred_img, cmap='binary_r')\n",
    "    ax[2].imshow(lab_unit, cmap='binary_r')\n",
    "\n",
    "    ax[0].set_title(f'Original Image | {actual_img_count + 1}')\n",
    "    ax[1].set_title(f'Prediction | {actual_img_count + 1}')\n",
    "    ax[2].set_title(f'Ground Truth | {actual_img_count + 1}')\n",
    "\n",
    "    # Remove tick labels for a cleaner plot\n",
    "    for a in ax:\n",
    "        a.set_yticklabels([])\n",
    "        a.set_xticklabels([])\n",
    "\n",
    "    #print(f'ID: {actual_img_count + 1} | IoU: {round(iou_score, 3)}')\n",
    "    \n",
    "    # Save selected images\n",
    "    #if actual_img_count in [15, 22, 29]:\n",
    "    #    plt.savefig(f'imgs/img{actual_img_count}.png', dpi=500, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the images\n",
    "height, width = pred_img.shape\n",
    "\n",
    "# Get corner pixel values for prediction\n",
    "pred_upper_left = pred_img[0, 0]\n",
    "pred_upper_right = pred_img[0, width - 1]\n",
    "pred_bottom_left = pred_img[height - 1, 0]\n",
    "pred_bottom_right = pred_img[height - 1, width - 1]\n",
    "\n",
    "# Get corner pixel values for ground truth\n",
    "lab_upper_left = lab_unit[0, 0]\n",
    "lab_upper_right = lab_unit[0, width - 1]\n",
    "lab_bottom_left = lab_unit[height - 1, 0]\n",
    "lab_bottom_right = lab_unit[height - 1, width - 1]\n",
    "\n",
    "# Print corner pixel values\n",
    "print(\"Prediction Corner Pixels:\")\n",
    "print(f\"Upper Left: {pred_upper_left}, Upper Right: {pred_upper_right}\")\n",
    "print(f\"Bottom Left: {pred_bottom_left}, Bottom Right: {pred_bottom_right}\")\n",
    "\n",
    "print(\"\\nGround Truth Corner Pixels:\")\n",
    "print(f\"Upper Left: {lab_upper_left}, Upper Right: {lab_upper_right}\")\n",
    "print(f\"Bottom Left: {lab_bottom_left}, Bottom Right: {lab_bottom_right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_errors_on_predictions_by_set(loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        log = []  # Overall log for all batches\n",
    "        total_non_blank_masks = 0\n",
    "        total_avg_iou = 0\n",
    "\n",
    "        # Loop through all batches in the DataLoader\n",
    "        for batch_idx, (inp, lab) in enumerate(loader):\n",
    "            inp = inp.to(DEVICE)\n",
    "            lab = lab.to(DEVICE)\n",
    "            pred = model(inp)\n",
    "\n",
    "            batch_size = inp.size(0)  # Number of images in the batch\n",
    "\n",
    "            # Loop through all images in the batch\n",
    "            for i in range(batch_size):\n",
    "                img_log = {}\n",
    "\n",
    "                lab_unit = lab[i, 0, :, :].cpu().numpy()  # Ground truth\n",
    "                inp_unit = inp[i].cpu().numpy()  # Input image\n",
    "                inp_unit = np.transpose(inp_unit[:3], (1, 2, 0))  # Adjust to HxWxC for RGB channels\n",
    "\n",
    "                pred_img = pred[i, 0, :, :].cpu().numpy()  # Prediction\n",
    "                pred_img = np.where(pred_img > 0.5, 1, 0)  # Threshold to binary\n",
    "\n",
    "                # Count TP, TN, FP, FN\n",
    "                TP = np.sum((pred_img == 1) & (lab_unit == 1))\n",
    "                TN = np.sum((pred_img == 0) & (lab_unit == 0))\n",
    "                FP = np.sum((pred_img == 1) & (lab_unit == 0))\n",
    "                FN = np.sum((pred_img == 0) & (lab_unit == 1))\n",
    "\n",
    "                # Initialize the log dictionary\n",
    "                img_log[\"TP\"] = TP\n",
    "                img_log[\"TN\"] = TN\n",
    "                img_log[\"FP\"] = FP\n",
    "                img_log[\"FN\"] = FN\n",
    "\n",
    "                img_log[\"pred_positive\"] = np.sum(pred_img == 1)\n",
    "                img_log[\"pred_negative\"] = np.sum(pred_img == 0)\n",
    "                img_log[\"lab_positive\"] = np.sum(lab_unit == 1)\n",
    "                img_log[\"lab_negative\"] = np.sum(lab_unit == 0)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "                recall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensitivity\n",
    "                specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "                precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                iou = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "\n",
    "                img_log[\"accuracy\"] = accuracy\n",
    "                img_log[\"recall\"] = recall\n",
    "                img_log[\"specificity\"] = specificity\n",
    "                img_log[\"precision\"] = precision\n",
    "                img_log[\"f1_score\"] = f1_score\n",
    "                img_log[\"iou\"] = iou\n",
    "\n",
    "                log.append(img_log)\n",
    "\n",
    "                # Print results for each image\n",
    "                print(f\"\\nBatch {batch_idx + 1}, Image {i + 1}/{batch_size}\")\n",
    "                print(\"Confusion Matrix:\")\n",
    "                print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "                print(\"Metrics:\")\n",
    "                print(f\"Accuracy: \\t\\t{accuracy:.4f}\")\n",
    "                print(f\"Recall (Sensitivity): \\t{recall:.4f}\")\n",
    "                print(f\"Specificity: \\t\\t{specificity:.4f}\")\n",
    "                print(f\"Precision: \\t\\t{precision:.4f}\")\n",
    "                print(f\"F1-Score: \\t\\t{f1_score:.4f}\")\n",
    "                print(f\"IoU: \\t\\t\\t{iou:.4f}\")\n",
    "\n",
    "                # Create an overlay with FP (yellow) and FN (green)\n",
    "                overlay = np.zeros((*pred_img.shape, 3), dtype=np.float32)  # Create a blank RGB image\n",
    "\n",
    "                # Assign colors for FP and FN\n",
    "                overlay[(pred_img == 1) & (lab_unit == 0)] = [1, 1, 0]  # Yellow for FP\n",
    "                overlay[(pred_img == 0) & (lab_unit == 1)] = [0, 1, 0]  # Green for FN\n",
    "\n",
    "                # Combine the overlay with a black-and-white version of the prediction\n",
    "                highlighted_pred = overlay  # Use the overlay directly for colored visualization\n",
    "\n",
    "                # Plot images for the current image in the batch\n",
    "                fig = plt.figure(figsize=(12, 15))  # Set overall figure size\n",
    "\n",
    "                # First row: Original image, Prediction, Ground Truth\n",
    "                ax1 = fig.add_subplot(2, 3, 1)\n",
    "                ax1.imshow((inp_unit - inp_unit.min()) / (inp_unit.max() - inp_unit.min()))  # Normalize for display\n",
    "                ax1.set_title(f'Original Image | {i + 1}')\n",
    "                ax1.axis('off')\n",
    "\n",
    "                ax2 = fig.add_subplot(2, 3, 2)\n",
    "                ax2.imshow(pred_img, cmap='binary_r')\n",
    "                ax2.set_title(f'Prediction | {i + 1}')\n",
    "                ax2.axis('off')\n",
    "\n",
    "                ax3 = fig.add_subplot(2, 3, 3)\n",
    "                ax3.imshow(lab_unit, cmap='binary_r')\n",
    "                ax3.set_title(f'Ground Truth | {i + 1}')\n",
    "                ax3.axis('off')\n",
    "\n",
    "                # Second row: Larger Prediction with FP and FN highlights\n",
    "                ax4 = fig.add_subplot(2, 1, 2)  # Spanning the entire width of the second row\n",
    "                ax4.imshow(highlighted_pred)\n",
    "                ax4.set_title(f'Prediction with FP (Yellow) & FN (Green) | {i + 1}')\n",
    "                ax4.axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # Update IoU calculation for non-blank masks\n",
    "                if img_log[\"lab_positive\"] != 0:\n",
    "                    total_non_blank_masks += 1\n",
    "                    total_avg_iou += img_log[\"iou\"]\n",
    "\n",
    "        # Final summary\n",
    "        print('\\n\\nimage:\\tP+\\tP-\\tM+\\tM-\\tTP\\tTN\\tFP\\tFN\\tacc\\trec\\tspec\\tprec\\tf1_s\\tiou')\n",
    "        for j, img_log in enumerate(log):\n",
    "            print(\n",
    "                f\" {j+1} \\t{img_log['pred_positive']}\\t{img_log['pred_negative']}\\t{img_log['lab_positive']}\\t{img_log['lab_negative']}\"\n",
    "                f\"\\t{img_log['TP']}\\t{img_log['TN']}\\t{img_log['FP']}\\t{img_log['FN']}\\t{img_log['accuracy']:.3f} \"\n",
    "                f\"\\t{img_log['recall']:.3f}\\t{img_log['specificity']:.3f}\\t{img_log['precision']:.3f}\"\n",
    "                f\"\\t{img_log['f1_score']:.3f}\\t{img_log['iou']:.3f}\"\n",
    "            )\n",
    "\n",
    "        print(f'\\nNb of non-blank masks: {total_non_blank_masks}, avg_iou: {(total_avg_iou / total_non_blank_masks):.6f}')\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_errors_on_predictions_by_set(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_errors_on_predictions_by_set(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_errors_on_predictions_by_set(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, pos_probs, neg_probs, nb_of_images_eval = compute_confusion_matrix_with_distributions(model, train_loader, DEVICE)\n",
    "display_confusion_matrix_with_metrics_and_distributions(cm, pos_probs, neg_probs, nb_of_images_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5b-Model-KNL",
   "language": "python",
   "name": "5b-model-knl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
